---
layout: post
title: "QANet-Combining Local Convolution with Global Self-Attention for Reading Comprehension"
mathjax: true
---

**Related Resources:**

- [Origional Paper](https://arxiv.org/abs/1804.09541)
- [Thang Luong's Slides](https://drive.google.com/file/d/1Mw6JZ9k0e8ajfiQ8uI-VP2my96DJINr4/view)
- [Kim's Blog (Following example code is taken from Kim's Blog)](https://medium.com/@minsangkim/implementing-question-answering-networks-with-cnns-5ae5f08e312b)
- [TensorFlow Implementation of QANet (By Kim)](https://github.com/NLPLearn/QANet)
- [PyTorch Implementation of QANet](https://github.com/BangLiu/QANet-PyTorch)

# 1 Abstract

> Ideas

- **Idea #1:** Mainstream MC models with *RNN and attention* are **slow** for both training and inference.
- **Idea #2:** In QANet, encoder only consists of *convolution and self-attention*.
- **Convolution models local interactions** and **self-attention models global interactions**.

## 1.1 LSTM is slow

Previous models like BiDAF uses a lot LSTM and is slow.

![](http://oi3xms5do.bkt.clouddn.com/BiDAF2.png)

## 1.2 Convolution & Self-Attention

Using convolution to capture local features.

![](http://oi3xms5do.bkt.clouddn.com/QANet_conv.png)

Using self-attention to capture global interactions.

![](http://oi3xms5do.bkt.clouddn.com/QANet_selfattn.png)

# 2 Introduction

Some background information about *Machine Reading Comprehension* can be found in the [R-Net Blog](http://fuyw.top/RNet).

Previous successful models generally employ two key ingredients:

1. RNN for sequential inputs.
2. Attention Mechanism for long term interactions.

However, these model are struggling for long time to train.In this paper: 

**1. Query and Context Encoder:** exclusivly use *convolutions and self-attentions* for *query and context encoder*.

**2. Context-Query Attention:** a standard attention is used to learn the interactions between context and questions.

**3. Model Encoder Layer:** resulting representation is encoded again with the recurrence-free encoder.

**4. Output Layer:** a task-specific layer, same stratege as in [Seo et al., (2016)](http://arxiv.org/abs/1611.01603.).

# 3 Model

## 3.1 Problem Formulation

Context paragraph with $$n$$ words:

$$
C = \lbrace c_1, c_2, \cdots, c_n \rbrace
$$

Query sentence with $$m$$ words:

$$
Q = \lbrace q_1, q_2, \cdots, q_m \rbrace
$$

Output span $$S$$:

$$
S = \lbrace{ c_i, c_{i+1}, \cdots, c_{i+j} \rbrace} 
$$

In the following, we'll use $$x$$ to denote both the original word and its embedded vector.

## 3.2 Model Overview

![](http://oi3xms5do.bkt.clouddn.com/QANet.png)

### 3.2.1 Input Embedding Layer

Embedding of each word $$w$$ is obtained by concatenating its *word embedding* and *character embedding*.

$$
\text{Embedding_of_word = word_embedding + character_embedding}
$$

where word embedding $$x_w \in \mathbb R^{p_1}$$ and character embedding $$x_c \in \mathbb R^{p_2}$$.

The output of a given word $$x$$ is the concatenation:

$$
\left[ x_w ; x_c \right] \in \mathbb R^{p_1 + p_2}
$$

[Kim](https://github.com/NLPLearn/QANet) implements the embedding layer with a similar approach as [(Kim et al., 2016)](https://arxiv.org/abs/1508.06615).

![](http://oi3xms5do.bkt.clouddn.com/CharCNN.png)

```python
import tensorflow as tf
import numpy as np

N = 32  # num_batch
PL = 400  # passage_length
QL = 30  # question_length
nh = 8  # number of heads in multi-head self attention. 
char_lim = 16  # character_limit
dim = 128  # dimension
dim_char = 32  # character_dimension
test_para_limit = 1000
test_ques_limit = 100
word_mat = np.load("glove.840B.300d.txt")

context = tf.placeholder(tf.int32, [None, PL], "context")
quesion = tf.placeholder(tf.int32, [None, QL], "question")
context_char = tf.placeholder(tf.int32, [None, PL, char_lim], "context_char")
question_char = tf.placeholder(tf.int32, [None, QL, char_lim], "question_char")
y1 = tf.placeholder(tf.int32, [None, PL], "answer_index1")
y2 = tf.placeholder(tf.int32, [None, PL], "answer_index2")

# Pretrained Word Vectors -- fixed during traing
word_mat = tf.get_variable("word_mat", initializer=tf.constant(
    word_mat, dtype=tf.float32), trainable=False)

# Character Vectors -- trainable
char_mat = tf.get_variable(
    "char_mat", initializer=tf.constant(char_mat, dtype=tf.float32))

# Context and Question Word's character embedding
context_char_emb = tf.reshape(tf.nn.embedding_lookup(
    char_mat, context_char), [N * PL, char_lim, dim_char])
quesion_char_emb = tf.reshape(tf.nn.embedding_lookup(
    char_mat, question_char), [N * QL, char_lim, dim_char])
```

`context_char_emb` and `question_char_emb` are put through a single layer of convolution and max-pooling. 

The concatenated `context_emb = tf.concat([context_emb, context_char_emb])` word embedding is then input to a highway network.

```python
# convolution
context_char_emb = conv(context_char_emb, dim,
                        bias=True, activation=tf.nn.relu, kernel_size=5, name="char_conv", reuse=None)
quesion_char_emb = conv(quesion_char_emb, dim,
                        bias=True, activation=tf.nn.relu, kernel_size=5, name="char_conv", reuse=True)

# max-pooling
context_char_emb = tf.reduce_max(context_char_emb, axis=1)
quesion_char_emb = tf.reduce_max(quesion_char_emb, axis=1)

context_char_emb = tf.reshape(context_char_emb, [N, PL, dim])
quesion_char_emb = tf.reshape(quesion_char_emb, [N, QL, dim])

# word embedding
context_emb = tf.nn.embedding_lookup(word_mat, context)
question_emb = tf.nn.embedding_lookup(word_mat, quesion)

# concatenation of word embedding and character embedding
context_emb = tf.concat([context_emb, context_char_emb], axis=2)
question_emb = tf.concat([question_emb, quesion_char_emb], axis=2)

# 2-layer highway network
context_emb = highway(context_emb, size=dim, layers=2, scope="highway", reuse=None)
question_emb = highway(question_emb, size=dim, layers=2, scope="highway", reuse=True)
```

### 3.2.2 Embedding Encoder Layer

Outputs of the embedding layer is input into the encoder layer to generate corresponding context and question representation.

![](http://oi3xms5do.bkt.clouddn.com/QANET1.png)

The encoder layer is a **stack** of basic building block:

$$
\left[\text{convolution-layer} \times \sharp + \text{self-attention-layer} + \text{feed-forward-layer} \right]
$$

and it uses **depthwise separable convolutions** [(Chollet., 2016)]( http://arxiv.org/abs/1610.02357 ) [(Kaiser et al., 2017)](https://arxiv.org/abs/1706.03059) rather than traditional ones. 

The kernel size is $$7$$, the number of filters is $$d=128$$ and the number of convert layers within a block is $$4$$.

For the **self-attention-layer**, we adopt the multi-head attention mechanism defined in [(Vaswani et al., 2017)](http://arxiv.org/abs/1706.03762 ).

For an input $$x$$ and a given operation $$f$$, the output is : 

$$
f(\text{layernorm}(x)) + x
$$

Note that the input of this layer is a vector of dimension $$p_1 + p_2 = 500$$ for each individual word, which is immediately mapped to $$d=128$$ by a one-dimensional convolution.

![](http://oi3xms5do.bkt.clouddn.com/QANET2.png)

### 3.2.3 Context-Query Attention Layer

![](http://oi3xms5do.bkt.clouddn.com/BiDAF3.png)

This module is **almost the same** as every previous reading comprehension models, such as **BiDAF**  [(Seo et al., 2016)](http://arxiv.org/abs/1611.01603.) and **DCN** [(Xiong et al., 2016)]( http://arxiv.org/abs/1611.01604) .

We use $$C \in \mathbb R^{d \times n}$$ and $$Q \in \mathbb R^{d \times m}$$ to denote the encoded context and query.

**Step 1. Calculate Similarity Matrix:**

Firstly, compute the similarites between each pair of context and query words, rendering a similarity matrix $$S \in \mathbb R^{n\times m}$$. 

$$
S  = \begin{pmatrix}
    f(q_1, c_1) &f(q_1, c_2) & \cdots & f(q_1, c_m) \\\\
    f(q_2, c_1) &f(q_2, c_2) & \cdots & f(q_2, c_m) \\\\
    \vdots & \vdots & \ddots & \vdots \\\\
    f(q_n, c_1) &f(q_n, c_2) & \cdots & f(q_n, c_m) 
\end{pmatrix} 
= \begin{pmatrix}
    s_{11} & s_{12} & \cdots & s_{1m} \\\\
    s_{21} & s_{22} & \cdots & s_{2m} \\\\
    \vdots & \vdots & \ddots & \vdots \\\\
    s_{n1} & s_{n2} & \cdots & s_{nm}
\end{pmatrix}
$$

Similarity function used here is the trilinear function:

$$
f(q, c) = W_0 \left[ q, c, q \odot c\right]
$$

where $$\odot$$ is the element-wise multiplication and $$W_0$$ is a trainable variable.

**Step 2. Calculate Context2Query Attention:**

Then, normalize each row of $$S$$ by applying the softmax function, getting a matrix $$\bar S$$. 

$$
\bar S = \begin{pmatrix}
\text {softmax} \left( s_{11} , s_{12}, \cdots, s_{1m} \right)  \\\\
\text {softmax} \left( s_{21} , s_{22}, \cdots, s_{2m} \right)  \\\\
\vdots \\\\
\text {softmax} \left( s_{n1} , s_{n2}, \cdots, s_{nm} \right)
\end{pmatrix} \in \mathbb R^{n \times m}
$$

The *Context-to-Query attention* is computed as $$A = \bar S \cdot Q^T \in \mathbb R^{n \times d}$$. 

$$
A = 
\begin{pmatrix}
    w_{11} & w_{12} & \cdots & w_{1m} \\\\
    w_{21} & w_{22} & \cdots & w_{2m} \\\\
    \vdots & \vdots & \ddots & \vdots \\\\
    w_{n1} & w_{n2} & \cdots & w_{nm}
\end{pmatrix}
\begin{pmatrix}
    q_{11} & q_{12} & \cdots & q_{1d} \\\\
    q_{21} & q_{22} & \cdots & q_{2d} \\\\
    \vdots & \vdots & \ddots & \vdots \\\\
    q_{m1} & q_{m2} & \cdots & q_{md}
\end{pmatrix}
= 
\begin{pmatrix}
    \sum \limits_ i w_{1i} q_{i1} & \sum \limits_ i w_{1i} q_{i2} & \cdots & \sum \limits_ i w_{1i} q_{id} \\\\
    \sum \limits_ i w_{2i} q_{i1} & \sum \limits_ i w_{2i} q_{i2} & \cdots & \sum \limits_ i w_{2i} q_{id} \\\\
    \vdots & \vdots & \ddots & \vdots \\\\
    \sum \limits_ i w_{ni} q_{i1} & \sum \limits_ i w_{ni} q_{i2} & \cdots & \sum \limits_ i w_{ni} q_{id}
\end{pmatrix}
$$

**Step 3. Calculate Query2Context Attention:**

For the *Query-to-Context attention*, QANet follows DCN attention [(Xiong et al., 2016)]( http://arxiv.org/abs/1611.01604). 

It computes the column normalized matrix $$\bar {\bar S} \in \mathbb R^{n \times m}$$ by softmax function, and the *query-to-context attention* $$B$$ is : 

$$
B = \bar S \cdot \bar {\bar S}^\top \cdot C^\top \in \mathbb R^{n \times d}
$$

### 3.2.4 Model Encoder Layer

Similar to [(Seo et al., 2016)](http://arxiv.org/abs/1611.01603.) the input of this layer at each position is $$\left[ c, a, c \odot a, c \odot b \right]$$, where $$a$$ and $$b$$ are respectively a row of attention matrix $$A$$ and $$B$$.

```python
context = residual_block(context_emb, num_blocks=1, num_conv_layers=4, kernel_size=7,
                         num_filters=dim, num_heads=nh, scope="Encoder_Residual_Block", bias=False)
question = residual_block(question_emb, num_blocks=1, num_conv_layers=4, kernel_size=7,
                          num_filters=dim, num_heads=nh, scope="Encoder_Residual_Block",
                          reuse=True,  # Share the weights between passage and question
                          bias=False)

tiled_context = tf.tile(tf.expand_dims(context, 2), [1, 1, QL, 1])
tiled_question = tf.tile(tf.expand_dims(question, 1), [1, PL, 1, 1])
S = trilinear([tiled_context, tiled_question, tiled_context * tiled_question])
S_ = tf.nn.softmax(S)
S_T = tf.transpose(tf.nn.softmax(S, dim=1), (0, 2, 1))
context2question = tf.matmul(S_, question)
question2context = tf.matmul(tf.matmul(S_, S_T), context)
attention_outputs = [context, context2question, context * context2question, context * question2context]
```

### 3.2.5 Output Layer

![](http://oi3xms5do.bkt.clouddn.com/QANET3.png)

Output layer is task-specific. 

For span selection, this paper follows the strategy of [(Seo et al., 2016)](http://arxiv.org/abs/1611.01603.) to predict the probability of each position in the context being the start or the end of an answer span.

$$
\begin{align}
    p^1 &= \text{softmax}(W_1 \left[ M_0 ; M_1 \right])  \\\\
    p^2 &= \text{softmax}(W_2 \left[ M_0; M_2 \right])
\end{align}
$$

where $$W_1$$ and $$W_2$$ are two trainable variables and $$M_0, M_1, M_2$$ are respectively the outputs of the three model encoders, from bottom to top.

The objective function is defined as the negative sum of the log probabilities of the predictied distributions indexed by true start and end indices, averaged over all the training examples:

$$
L(\theta) = - \frac 1 N \sum_{i}^N \left[ \log(p^1_{y^1_i}) + \log(p^2_{y^2_i}) \right]
$$

```python
inputs = tf.concat(attention_outputs, axis=-1)
encoder_inputs = [conv(inputs, dim, name="input_projection")]

for i in range(3):
    encoder_inputs.append(
        residual_block(encoder_inputs[i],
                       num_blocks=7,
                       num_conv_layers=2,
                       kernel_size=5,
                       num_filters=dim,
                       num_heads=nh,
                       scope="Model_Encoder",
                       bias=False,
                       reuse=True if i > 0 else None)
    )

# only use the first and second output of stacked encoder for the first answer probability calculation
start_prob = tf.squeeze(conv(tf.concat([encoder_inputs[1], encoder_inputs[2]], axis=-1), 1, bias=False, name="start_pointer"), -1)

# use the first and THIRD output of stacked encoder for the last answer probability calculation
end_prob = tf.squeeze(conv(tf.concat([encoder_inputs[1], encoder_inputs[3]], axis=-1), 1, bias=False, name="end_pointer"), -1)

# We calculate the loss
losses = tf.nn.softmax_cross_entropy_with_logits(logits=start_prob, labels=y1)
losses2 = tf.nn.softmax_cross_entropy_with_logits(logits=end_prob, labels=y2)
loss = tf.reduce_mean(losses + losses2)
```
