---
layout: post
title: "Xception: Deep Learning with Depthwise Separable Convolutions"
mathjax: true
---

**Reference:**

- Inspiration of Inception: [Network-In-Network](https://arxiv.org/abs/1312.4400)
- GoogLeNet (Inception V1): [Going depper with convolutions. (Szegedy et al. 2014)](https://arxiv.org/abs/1409.4842)
- Inception V2: [Batch normalization: Accelerating deep network training by reducing internal covariate shift. (Ioffe et al. 2015)](https://arxiv.org/abs/1502.03167)
- Inception V3: [Rethinking the inception architecture for computer vision. (Szegedy et al. 2015)](https://arxiv.org/abs/1512.00567)
- Inception-ResNet (Inception V4): [Inception-v4, inception-resnet and the impact of residual connections on learning. (Szegedy et al. 2016)](https://arxiv.org/abs/1602.07261)
- Xception: [Xception: Deep Learning with Depthwise Separable Convolutions. (Chollet 2016)](https://arxiv.org/abs/1610.02357)
- [François Chollet Blog](https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704)
- [François Chollet's Interview](https://www.pyimagesearch.com/2018/07/02/an-interview-with-francois-chollet/)

# 1 Xception

Inception modules are conceptually similar to convolutions (convolutional feature extractors), they empirically appear to be capable of learning richer representations with less parameters.

**Q1: How do they work?**

**Q2: How do they differ from regular convolutions?**

**Q3: What design strategies come after Inception?**

## 1.1 The Inception hypothesis

传统的卷积核要同时顾及 cross-channel correlation 和 spatial correlation 两方面。

A convolution layer attempts to learn filters in a 3D space, with 2 spatial dimensions (width and height) and a channel dimension; thus **a single convolution kernel is tasked with simultaneously mapping cross-channel correlations and spatial correlations**.

Chollet 认为 Inception 的理念是 —— 分而治之 (但 Inception V1 的出发点并不是这个)。

*The idea behind the Inception module is to make this process easier and more efficient by explicitly facotring it into a series of operations that would independently look at cross-channel correlations and at spatial correlations.*

*Typically, the Inception module first looks as cross-channel correlations via a set of 1x1 convolutions, **mapping the input data into 3 or 4 seperate spaces that are smaller than the original input space**, and then maps all correlations in these smaller 3D spaces, via regular 3x3 or 5x5 convolutions.*

**基础假设:**

Cross-channel correlations and spatial correlations are sufficiently decoupled that it is preferable not to map them jointly.

![](http://oi3xms5do.bkt.clouddn.com/canonical_inception.png)

Consider a simplified version of an Inception module that only uses one size of convolution (e.g. 3x3) and does not include an average pooling tower.

![](http://oi3xms5do.bkt.clouddn.com/simplified_inception.png)

This Inception module can be reformulated as a large 1x1 convolution followed by spatial convolutions that would operate on non-overlapping segments of the output channels.

![](http://oi3xms5do.bkt.clouddn.com/reformulation_inception.png)

> Question

- What is the effect of the number of segments in the partition (and their size)?

- Would it be reasonable to make a much stronger hypothesis than the Inception hypothesis, and assume that cross-channel correlations and spatial correlations can be mapped completed separately?

## 1.2 The continuum between convolutions and seperable convolutions

An "extreme" version of an Inception module, based on this stronger hypothesis, would first use a 1x1 convolution to map cross-channel correlations, and would then separately map the spatial correlations of every output channel.

![](http://oi3xms5do.bkt.clouddn.com/extreme_inception.png)

This extreme form of an Inception module is almost identical to a *depthwise separable convolution*.

A depthwise separable convolution, commonly called "separable convolution" in TensorFlow and Keras, consists in a *depthwise convolution*, i.e. a spatial convolution performed independently over each channel of an input, followed by a *pointwise convolution*, i.e. a 1x1 convolution, projecting the channels output by the depthwise convolution onto a new channel space.

> Minor differences

- **Order of operations:** depthwise separable convolutions as usually implemented perform first channel-wise spatial convolution and then perform 1x1 convolution, whereas Inception performs the 1x1 convolution first.
- The presence or absence of a non-linearity after the first operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise separable convolutions are usually implemented without non-linearities.

The first difference is unimportant, in particular because thses operations are meant to be used in a stacked setting.

The second difference might matter, and this paper investigate it in the experimental section.

## 1.3 Separable Convolution

![](http://oi3xms5do.bkt.clouddn.com/sep_conv.png)

## 1.4 The Xception architecture

Xception(Extreme Inception) is a convolutional neural network architecture based entirely on depthwise separable convolution layers.

It hypothesize that: the mapping of cross-channels correlations and spatial correlations in the feature maps of convolutional neural networks can be *entirely* decoupled.

**In short, the Xception architecture is a linear stack of depthwise separable convolution layers with residual connections.**

![](http://oi3xms5do.bkt.clouddn.com/Xception.png)

```python
# (10, 299, 299, 3)
img_input = tf.constant(np.random.rand(10, 299, 299, 3))

# (10, 149, 149, 32)
x = layers.Conv2D(32, (3, 3), strides=(2,2), use_bias=False, name='block1_conv1')(img_input)
x = layers.BatchNormalization(name='block1_conv1_bn')(x)
x = layers.Activation('relu', name='block1_conv1_act')(x)

# (10, 147, 147, 64)
x = layers.Conv2D(64, (3, 3), use_bias=False, name='block1_conv2')(x)
x = layers.BatchNormalization(name='block1_conv2_bn')(x)
x = layers.Activation('relu', name='block1_conv2_act')(x)

# (10, 74, 74, 128)
residual = layers.Conv2D(128, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
residual = layers.BatchNormalization()(residual)

# (10, 147, 147, 128)
x = layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')(x)
x = layers.BatchNormalization(name='block2_sepconv1_bn')(x)
x = layers.Activation('relu', name='block2_sepconv2_act')(x)

# (10, 147, 147, 128)
x = layers.SeparableConv2D(128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')(x)
x = layers.BatchNormalization(name='block2_sepconv2_bn')(x)

# (10, 74, 74, 128)
x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)

# (10, 74, 74, 128)
x = layers.add([x, residual])

# (10, 37, 37, 256)
residual = layers.Conv2D(256, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
residual = layers.BatchNormalization()(residual)

# (10, 74, 74, 128)
x = layers.Activation('relu', name='block3_sepconv1_act')(x)

# (10, 74, 74, 256)
x = layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')(x)
x = layers.BatchNormalization(name='block3_sepconv1_bn')(x)
x = layers.Activation('relu', name='block3_sepconv2_act')(x)

# (10, 74, 74, 256)
x = layers.SeparableConv2D(256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')(x)
x = layers.BatchNormalization(name='block3_sepconv2_bn')(x)

# (10, 37, 37, 256)
x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)
x = layers.add([x, residual])

# (10, 19, 19, 728)
residual = layers.Conv2D(728, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
residual = layers.BatchNormalization()(residual)

x = layers.Activation('relu', name='block4_sepconv1_act')(x)
x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')(x)
x = layers.BatchNormalization(name='block4_sepconv1_bn')(x)
x = layers.Activation('relu', name='block4_sepconv2_act')(x)
x = layers.SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')(x)
x = layers.BatchNormalization(name='block4_sepconv2_bn')(x)

x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)
x = layers.add([x, residual])

for i in range(8):
    residual = x
    prefix = 'block' + str(i + 5)

    x = layers.Activation('relu', name=prefix + '_sepconv1_act')(x)
    x = layers.SeparableConv2D(728, (3, 3), 
                               padding='same', 
                               use_bias=False, 
                               name=prefix + '_sepconv1')(x)
    x = layers.BatchNormalization(name=prefix + '_sepconv1_bn')(x)
    x = layers.Activation('relu', name=prefix + '_sepconv2_act')(x)
    x = layers.SeparableConv2D(728, (3, 3),
                               padding='same',
                               use_bias=False,
                               name=prefix + '_sepconv2')(x)
    x = layers.BatchNormalization(name=prefix + '_sepconv2_bn')(x)
    x = layers.Activation('relu', name=prefix + '_sepconv3_act')(x)
    x = layers.SeparableConv2D(728, (3, 3),
                               padding='same',
                               use_bias=False,
                               name=prefix + '_sepconv3')(x)
    x = layers.BatchNormalization(name=prefix + '_sepconv3_bn')(x)

    x = layers.add([x, residual])

residual = layers.Conv2D(1024, (1, 1), strides=(2, 2),
                             padding='same', use_bias=False)(x)
residual = layers.BatchNormalization()(residual)

x = layers.Activation('relu', name='block13_sepconv1_act')(x)
x = layers.SeparableConv2D(728, (3, 3),
                           padding='same',
                           use_bias=False,
                           name='block13_sepconv1')(x)
x = layers.BatchNormalization(name='block13_sepconv1_bn')(x)
x = layers.Activation('relu', name='block13_sepconv2_act')(x)
x = layers.SeparableConv2D(1024, (3, 3),
                           padding='same',
                           use_bias=False,
                           name='block13_sepconv2')(x)
x = layers.BatchNormalization(name='block13_sepconv2_bn')(x)

x = layers.MaxPooling2D((3, 3),
                        strides=(2, 2),
                        padding='same',
                        name='block13_pool')(x)
x = layers.add([x, residual])

x = layers.SeparableConv2D(1536, (3, 3),
                           padding='same',
                           use_bias=False,
                           name='block14_sepconv1')(x)
x = layers.BatchNormalization(name='block14_sepconv1_bn')(x)
x = layers.Activation('relu', name='block14_sepconv1_act')(x)

x = layers.SeparableConv2D(2048, (3, 3),
                           padding='same',
                           use_bias=False,
                           name='block14_sepconv2')(x)
x = layers.BatchNormalization(name='block14_sepconv2_bn')(x)
x = layers.Activation('relu', name='block14_sepconv2_act')(x)

x = layers.GlobalAveragePooling2D(name='avg_pool')(x)
x = layers.Dense(classes, activation='softmax', name='predictions')(x)
```

# 2 Inception-v1

[(Szegedy et al., CVPR' 15)](https://arxiv.org/abs/1409.4842) 提出 **GoogLeNet** (Inception-v1) ，比 AlexNet 少 12 倍的参数，且效果更好。

Inception 浑身散发 “实用主义” 气息，它考虑问题的出发点是 **practical**：因为**计算资源有限**、**标记数据集不足**，所以要求**模型参数数量**不能太多。

*The network was designed with computational efficiency and practically in mind.*

## 2.1 Network-in-Network

[(Lin et al., 2013)](https://arxiv.org/abs/1312.4400) proposed **Network-in-Network**: 

- Increase the representational power of neural networks. (证据不足?)
- A **dimension reduction** modules to remove computational bottlenecks. (模型性能)

### 2.1.1 Problems

网络的结构越复杂，容易过拟合，尤其在训练集数据大小有限情况下：

**bigger size network** ==> **large number of parameters** ==> **overfitting** ==> **not enough labeled data**

### 2.1.2 Motivation

**Dimension Reduction:** Shrink the number of channels，reduce the number of multiply operations.

对 28x28x192 的输入，如果直接上 5x5x32 的卷积核，需要 120M 的乘法操作。而先加一个 1x1x16 的卷积核将输入压缩成 28x28x16 再上 5x5x32 的卷积核，只需要 12.4M 的乘法操作。

几乎减少了十倍 (虽然 12.4M 直观上已经是个可怕的数量级)。

![](http://oi3xms5do.bkt.clouddn.com/Conv1_1.png)

![](http://oi3xms5do.bkt.clouddn.com/Conv1_2.png)

## 2.2 Inception Module

### 2.2.1  Szegedy's explanation

Szegedy 在论文中的解释是：因为计算资源有限、标记数据不足，限制了模型的 size，需要引入 **sparsity**。

*Introduce sparsity and replace the fully connected layers by the sparse ones, even inside the convolutions.*

接着 Szegedy 指出一个难点(没看懂)：

*Unfortunately, today’s computing infrastructures are very inefficient when it comes to numerical calculation on non-uniform sparse data structures.*

于是他提出从 filter-level 引入 sparsity：

*an architecture that makes use of filter-level sparsity*

以及如何通过现有的 dense components 来构建一个 parse convolution networks：

*The main idea of the Inception architecture is to consider how an optimal local sparse structure of a convolutional vision network can be approximated and covered by readily available dense components.*

由于要考虑平移不变形( translation invariance)，所以还是需要使用卷积核：

*Note that assuming translation invariance means that our network will be built from convolutional building blocks.*

至于 1x1，3x3 和 5x5 的卷积核选择，则更多是出于方便性的考虑。

![](http://oi3xms5do.bkt.clouddn.com/Szegedy_v1.png)

上图可以表示 Inception-v1 的两个精髓思想：**filter-level sparsity** & **dimensionality reduction**

**(a)** 是对 filter-level sparsity 的 naive 实现，把 1x1，3x3，5x5 和 MaxPool 直接拼起来。

**(b)** 是出于计算量考虑，在 3x3，5x5 前，在 MaxPool 后加上了 1x1 的卷积核，达到 dimensionality reduction 效果。另一点是 Szegedy claims that inspired by **embeddings**: even low dimensional embeddings might contain a lot of information about a relatively large image patch.

### 2.2.2 Andrew Ng's explanation

吴恩达的解释更加简略：以前的网络需要人们手工选择卷积核的尺寸，而 Inception Network 中 instead of choosing what filter size you want in conv layer, let's do them all.

同时在 Previous Activation (28x28x192) 上用 1x1 CONV, 3x3 CONV, 5x5 CONV 和 MaxPool，出于减少计算量原因，在 3x3 CONV 和 5x5 CONV 前要加一个 1x1 CONV。为了让 MaxPool 得到的 layer 少一点，在 MaxPool 后面接一个 1x1 CONV。

![](http://oi3xms5do.bkt.clouddn.com/Andrew_incm.png)

## 2.3 Inception Network

Inception Network 就是把多个 Inception Module 拼起来，其中会使用 MaxPool 使输入尺寸减半：

*In general, an Inception network is a network consisting of modules of the above type stacked upon each other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid.*

![](http://oi3xms5do.bkt.clouddn.com/Andrew_inception.png)

# 3 Inception-v2

[(Ioffe et al., ICML' 15)](https://arxiv.org/abs/1502.03167) 提出 Batch Normalization，发现可以大大加快模型训练时间 (可以选更大的 lr)，以及提高准确性。

Ioffe 在 Inception Network 上加了 BN 层得到 Inception-v2 (另外一点是把 5x5 的卷积核用两个连续的 3x3 卷积核替换)。

## 3.1 Internal Covariate Shift

模型层数越多，对输入参数的微小变动都可能造成输出结果的剧烈变化。

The training is complicated by the fact that the input to each layer are affected by the parameters of all preceding layers -- so that small changes to the network parameters amplify as the network becomes deeper.

The change in the distributions of layers' inputs presents a problem because the layers need to continuously adapt to the new distribution.

用 sigmoid 做激活函数时，当输入 x 的绝对值很大，会出现梯度消失，训练收敛过慢的问题。对于一个较深的模型，在后面几层就很容易出现这种问题。虽然可以用 ReLU 、careful initialization 和 small learning rates 解决，但如果我们可以确保：

*distribution of nonlinearity inputs remains more stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime, and the training would accelerate.*

Ioffe 把输入的分布变化称作 Internal Covariate Shift：

*We define Internal Covariate Shift as the change in th distribution of network activations due to the change in network parameters during training.*

然后 BN 的目的就是使得训练时得到一个稳定的  distribution of activation values：

**The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training**

## 3.2 Whitened Inputs

[(LeCun et al., 1998)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) and [(Wiesler & Ney, 2011)](https://papers.nips.cc/paper/4421-a-convergence-analysis-of-log-linear-training) showed that the network training converges faster if its inputs are whitened：

*linearly transformed to have zero means and unit variances, and decorrelated.*

因为每一层都是接收上一层的输入，因此需要对每一层的 input 做同样的 whitening 操作。

*By whitening the inputs to each layer, we would take a step towards achieving the fixed distributions of inputs that would remove the ill effects of the internal covariate shift.*

## 3.3 Batch Normalization

具体细节略，可参考 [MXNet 教程](http://zh.gluon.ai/chapter_convolutional-neural-networks/batch-norm.html)。

```python
def conv2d_bn(x, filters, num_row, num_col, padding='same', strides=(1, 1)):
    x = Conv2D(filters, (num_row, num_col), strides=strides, padding=padding, use_bias=False)(x)
    x = BatchNormalization(axis=3, scale=False)(x)
    x = Activation('relu')(x)
    return x
```

# 4 Inception-v3

[(Szegedy et al., CVPR' 16)](https://arxiv.org/abs/1512.00567) 采用 factorized convolutions 的方法改变了 Inception Modulde 的结构，得到 Inception-v3。

Szegedy 的出发点还是如何在计算资源受限的条件下，increase model size ==> Scaling up convolution networks in efficient ways.

## 4.1 General Design Principles

Szegedy 在文中总结了几条炼丹的经验，他坦言这些经验是 speculative 需要 additional future experimental evidence 来验证。

然后整个 v3 很多结构都是根据这几条丹方设计的，太玄学，不喜欢。

1. Avoid representational bottlenecks, especially early in the network. (没看懂)
2. Higher dimensional representations are easier to process locally within a network. (没看懂)
3. Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power.
4. Balance the width and depth of the network.

## 4.2 Factorizing Convolutions

Szegedy 的出发点都是从 CV 来的，值得注意的是：不能盲目套用，削足适履，要看应用场景是否符合模型的前提假设。

In a vision network, it is expected that the outputs of near-by activations are highly correlated. Therefore, we can expect that their activations can be reduced before aggregation and that this should result in similarity expressive local representations.

因为大尺寸的卷积核计算开销高，比如 5x5 的卷积核比 3x3 的卷积核计算量大 25/9 = 2.78 倍。

Szegedy 考虑是否能用更少参数的结构来替换 5x5 的卷积核：

If we zoom into the computation graph of the 5 × 5 convolution, we see that each output looks like a small fully-connected network sliding over 5 × 5 tiles over its input (see Figure 1).

![](http://oi3xms5do.bkt.clouddn.com/factorize_conv.png)

Since we are constructing a vision network, it seems natural to exploit translation invariance again and replace the fully connected component by a two layer convolutional architecture: the first layer is a 3 × 3 convolution, the second is a
fully connected layer on top of the 3 × 3 output grid of the first layer (see Figure 1).

Sliding this small network over the input activation grid boils down to replacing the 5 × 5 convolution with two layers of 3 × 3 convolution (compare Figure 4 with 5).

![](http://oi3xms5do.bkt.clouddn.com/5x5_3x3.png)

## 4.3 Split in Asymmetric Convolutions

Further, we can use a 3 × 1 convolution followed by a 1 × 3 convolution is equivalent to sliding a two layer network with the same receptive field as in a 3 × 3 convolution (see figure 3).

![](http://oi3xms5do.bkt.clouddn.com/nx1.png)

In theory, we could go even further and argue that one can replace any n × n convolution by a 1 × n convolution followed by a n × 1 convolution and the computational cost saving increases dramatically as n grows (see figure 6).

![](http://oi3xms5do.bkt.clouddn.com/17_1xn.png)

## 4.4 Inception-v3

![](http://oi3xms5do.bkt.clouddn.com/incv_3.png)

我有个很大的疑问：在 1x1 CONV 之前 x 的 channel 数为什么要从 60 增加到 80 再增加到 192？

这样做看起来就像是为了用 Dimension Reduction 而刻意这么做的，完全无法理解为什么要突然把 channel 的数量突然提到那么大。

而且不同层里面 filter 的个数更无法理解，这方面的解释性差了很多，让人不爽。

```python
# keras 中实现的 inception-v3 结构上和论文中的有些不同

# (10, 299, 299, 3) --> (10, 149, 149, 32)
x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')
x = conv2d_bn(x, 32, 3, 3, padding='valid')      # (10, 147, 147, 32)
x = conv2d_bn(x, 64, 3, 3)                       # (10, 147, 147, 64)
x = MaxPooling2D((3, 3), strides=(2, 2))(x)      # (10, 73, 73, 64)

x = conv2d_bn(x, 80, 1, 1, padding='valid')      # (10, 73, 73, 80)
x = conv2d_bn(x, 192, 3, 3, padding='valid')     # (10, 71, 71, 192)
x = MaxPooling2D((3, 3), strides=(2, 2))(x)      # (10, 35, 35, 192)

# mixed 0, 1, 2: 35 x 35 x 256
branch1x1 = conv2d_bn(x, 64, 1, 1)               # (10, 35, 35, 64)

branch5x5 = conv2d_bn(x, 48, 1, 1)               # (10, 35, 35, 48)
branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)       # (10, 35, 35, 64)

branch3x3dbl = conv2d_bn(x, 64, 1, 1)            # (10, 35, 35, 64)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) # (10, 35, 35, 96)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) # (10, 35, 35, 96)


branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)    # (10, 35, 35, 192)
branch_pool = conv2d_bn(branch_pool, 32, 1, 1)   # (10, 35, 35, 32)
x = layers.concatenate(
    [branch1x1, branch5x5, branch3x3dbl, branch_pool],
    axis=channel_axis,
    name='mixed0')                               # (10, 35, 35, 256)

# mixed 1: 35 x 35 x 288
branch1x1 = conv2d_bn(x, 64, 1, 1)               # (10, 35, 35, 64)

branch5x5 = conv2d_bn(x, 48, 1, 1)               # (10, 35, 35, 48)
branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)       # (10, 35, 35, 64)

branch3x3dbl = conv2d_bn(x, 64, 1, 1)            # (10, 35, 35, 64)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) # (10, 35, 35, 96)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3) # (10, 35, 35, 96)

branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)    # (10, 35, 35, 256)
branch_pool = conv2d_bn(branch_pool, 64, 1, 1)   # (10, 35, 35, 64)
x = layers.concatenate(
    [branch1x1, branch5x5, branch3x3dbl, branch_pool],
    axis=channel_axis,
    name='mixed1')                               # (10, 35, 35, 288)

# mixed 2: 35 x 35 x 256
branch1x1 = conv2d_bn(x, 64, 1, 1)

branch5x5 = conv2d_bn(x, 48, 1, 1)
branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)

branch3x3dbl = conv2d_bn(x, 64, 1, 1)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)

branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)
branch_pool = conv2d_bn(branch_pool, 64, 1, 1)
x = layers.concatenate(
    [branch1x1, branch5x5, branch3x3dbl, branch_pool],
    axis=channel_axis,
    name='mixed2')

# mixed 3: 17 x 17 x 768
branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding='valid')

branch3x3dbl = conv2d_bn(x, 64, 1, 1)
branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)
branch3x3dbl = conv2d_bn(
    branch3x3dbl, 96, 3, 3, strides=(2, 2), padding='valid')

branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)
x = layers.concatenate(
    [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name='mixed3')

# mixed 4: 17 x 17 x 768
branch1x1 = conv2d_bn(x, 192, 1, 1)

branch7x7 = conv2d_bn(x, 128, 1, 1)
branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)
branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)

branch7x7dbl = conv2d_bn(x, 128, 1, 1)
branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)
branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)
branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)
branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)

branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)
branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
x = layers.concatenate(
    [branch1x1, branch7x7, branch7x7dbl, branch_pool],
    axis=channel_axis,
    name='mixed4')

# mixed 5, 6: 17 x 17 x 768
for i in range(2):
    branch1x1 = conv2d_bn(x, 192, 1, 1)

    branch7x7 = conv2d_bn(x, 160, 1, 1)
    branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)
    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)

    branch7x7dbl = conv2d_bn(x, 160, 1, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)
    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)

    branch_pool = AveragePooling2D(
        (3, 3), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch7x7, branch7x7dbl, branch_pool],
        axis=channel_axis,
        name='mixed' + str(5 + i))

# mixed 7: 17 x 17 x 768
branch1x1 = conv2d_bn(x, 192, 1, 1)

branch7x7 = conv2d_bn(x, 192, 1, 1)
branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)
branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)

branch7x7dbl = conv2d_bn(x, 192, 1, 1)
branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)
branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)
branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)
branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)

branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)
branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
x = layers.concatenate(
    [branch1x1, branch7x7, branch7x7dbl, branch_pool],
    axis=channel_axis,
    name='mixed7')

# mixed 8: 8 x 8 x 1280
branch3x3 = conv2d_bn(x, 192, 1, 1)
branch3x3 = conv2d_bn(branch3x3, 320, 3, 3,
                      strides=(2, 2), padding='valid')

branch7x7x3 = conv2d_bn(x, 192, 1, 1)
branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)
branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)
branch7x7x3 = conv2d_bn(
    branch7x7x3, 192, 3, 3, strides=(2, 2), padding='valid')

branch_pool = MaxPooling2D((3, 3), strides=(2, 2))(x)
x = layers.concatenate(
    [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name='mixed8')

# mixed 9: 8 x 8 x 2048
for i in range(2):
    branch1x1 = conv2d_bn(x, 320, 1, 1)

    branch3x3 = conv2d_bn(x, 384, 1, 1)
    branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)
    branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)
    branch3x3 = layers.concatenate(
        [branch3x3_1, branch3x3_2], axis=channel_axis, name='mixed9_' + str(i))

    branch3x3dbl = conv2d_bn(x, 448, 1, 1)
    branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)
    branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)
    branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)
    branch3x3dbl = layers.concatenate(
        [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis)

    branch_pool = AveragePooling2D(
        (3, 3), strides=(1, 1), padding='same')(x)
    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)
    x = layers.concatenate(
        [branch1x1, branch3x3, branch3x3dbl, branch_pool],
        axis=channel_axis,
        name='mixed' + str(9 + i))

# Classification block
x = GlobalAveragePooling2D(name='avg_pool')(x)
x = Dense(classes, activation='softmax', name='predictions')(x)
```

# 5 Inception-v4

[(Szegedy et al., AAAI' 17)](https://arxiv.org/abs/1602.07261) 中提出了精简后的 Inception-v4 及 Inception-ResNet-v1、Inception-ResNet-v2。

Inception-ResNet 的想法非常直接：Inception + ResNet，并通过实证发现 residual connections 可以加快模型训练时间，明显提升模型效果。

## 5.1 Inception & ResNet

**Observasions:**

1. [He et al. 2015]() points out that residual  connections are inherently important for training very deep architectures.

2. Inception networks tend to be very deep.

**Solutions:**

Replace the filter concatenation stage of the Inception architecture with residual connections.

![](http://oi3xms5do.bkt.clouddn.com/Residual_connect.png)

## 5.2 Architecture

Inception architecture is highly tunable, meaning that there are a lot of possible changes to the number of filters in the various layers that do not affect the quality of the fully trained network. ==> 是否意味着各层中 number of filters 的作用不大？

在 v4 版之前，出于 memory restriction 的限制 (非 TF 实现，Memory 使用效率低，有一些非必要的限制) ==> Not simplifying earlier choices resulted in networks that looked more complicated that they needed to be。 

Inception-v4 we decided to shed this unnecessary baggage and made uniform choices for the Inception blocks for each grid size.

### 5.2.1 Inception Block

Each inception block is followed by filter-expansion layer (1x1 convolution without activation) which is used for scaling uo the dimensionality of the filter bank before the residual addition to match the depth of the input. This is needed to compensate for the dimensionality reduction induced by the Inception block.

![](http://oi3xms5do.bkt.clouddn.com/inceptionv4.png)

其中三个不同的 Inception Module:

![](http://oi3xms5do.bkt.clouddn.com/inceptionv41.png)

### 5.2.2 Inception ResNet

Overall schema for Inception-Resnet-v1(v2):

![](http://oi3xms5do.bkt.clouddn.com/inceptionv12.png)

Interior grid modules of Inception-ResNet Networks:

![](http://oi3xms5do.bkt.clouddn.com/IncRes12.png)